{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bea3becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: \"Baby Shark Dance\"[7], Name: Pinkfong Baby Shark - Kids' Songs & Stories, Artist: 7,046,700,000, Upload Date: June 17, 2016, Views: November 2, 2020\n",
      "Rank: \"Despacito\"[10], Name: Luis Fonsi, Artist: 2,993,700,000, Upload Date: January 12, 2017, Views: August 4, 2017\n",
      "Rank: \"See You Again\"[20], Name: Wiz Khalifa, Artist: 2,894,000,000, Upload Date: April 6, 2015, Views: July 10, 2017\n",
      "Rank: \"Gangnam Style\"‚ÅÇ[31], Name: Psy, Artist: 803,700,000, Upload Date: July 15, 2012, Views: November 24, 2012\n",
      "Rank: \"Baby\"*[69], Name: Justin Bieber, Artist: 245,400,000, Upload Date: February 19, 2010, Views: July 16, 2010\n",
      "Rank: \"Bad Romance\"[73], Name: Lady Gaga, Artist: 178,400,000, Upload Date: November 24, 2009, Views: April 14, 2010\n",
      "Rank: \"Charlie Bit My Finger\"[77], Name: HDCYT, Artist: 128,900,000, Upload Date: May 22, 2007, Views: October 25, 2009\n",
      "Rank: \"Evolution of Dance\"[79], Name: Judson Laipply, Artist: 118,900,000, Upload Date: April 6, 2006, Views: May 2, 2009\n",
      "Rank: \"Girlfriend\"‚Ä°[81][82], Name: RCA Records, Artist: 92,600,000, Upload Date: February 27, 2007, Views: July 17, 2008\n",
      "Rank: \"Evolution of Dance\"[79], Name: Judson Laipply, Artist: 78,400,000, Upload Date: April 6, 2006, Views: March 15, 2008\n",
      "Rank: \"Music Is My Hot Hot Sex\"‚Ä°[87], Name: CLARUSBARTEL72, Artist: 76,600,000, Upload Date: April 9, 2007, Views: March 1, 2008\n",
      "Rank: \"Evolution of Dance\"*[79], Name: Judson Laipply, Artist: 10,600,000, Upload Date: April 6, 2006, Views: May 19, 2006\n",
      "Rank: \"Pok√©mon Theme Music Video\"‚Ä°[92], Name: Smosh, Artist: 4,300,000, Upload Date: November 28, 2005, Views: March 12, 2006\n",
      "Rank: \"Myspace ‚Äì The Movie\"‚Ä°[97][98], Name: eggtea, Artist: 2,700,000, Upload Date: January 31, 2006, Views: February 18, 2006\n",
      "Rank: \"Phony Photo Booth\"‚Ä°[101], Name: mugenized, Artist: 3,400,000, Upload Date: December 1, 2005, Views: January 21, 2006\n",
      "Rank: \"The Chronic of Narnia Rap\"‚Ä°[107], Name: youtubedude, Artist: 2,300,000, Upload Date: December 18, 2005, Views: January 9, 2006\n",
      "Rank: \"Ronaldinho: Touch of Gold\"‚Ä°*[110], Name: Nikesoccer, Artist: 255,000, Upload Date: October 21, 2005, Views: October 31, 2005\n",
      "Rank: \"I/O Brush\"‚Ä°*[116], Name: larfus, Artist: 247,000, Upload Date: October 5, 2005, Views: October 29, 2005\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "rows = table.find_all('tr')[1:]\n",
    "\n",
    "most_viewed_videos = []\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols) >= 5: \n",
    "        rank = cols[0].text.strip()\n",
    "        name = cols[1].text.strip()\n",
    "        artist = cols[2].text.strip()\n",
    "        upload_date = cols[3].text.strip()\n",
    "        views = cols[4].text.strip()\n",
    "        most_viewed_videos.append({\n",
    "            'Rank': rank,\n",
    "            'Name': name,\n",
    "            'Artist': artist,\n",
    "            'Upload Date': upload_date,\n",
    "            'Views': views\n",
    "        })\n",
    "\n",
    "for video in most_viewed_videos:\n",
    "    print(f\"Rank: {video['Rank']}, Name: {video['Name']}, Artist: {video['Artist']}, Upload Date: {video['Upload Date']}, Views: {video['Views']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd648ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BCCI website\n",
    "url = 'https://www.bcci.tv/'\n",
    "\n",
    "# Create a session\n",
    "session = requests.Session()\n",
    "\n",
    "# Fetch the home page\n",
    "home_page = session.get(url)\n",
    "home_soup = BeautifulSoup(home_page.content, 'html.parser')\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_link = None\n",
    "for link in home_soup.find_all('a', href=True):\n",
    "    if 'international' in link['href']:\n",
    "        fixtures_link = link['href']\n",
    "        break\n",
    "\n",
    "if fixtures_link:\n",
    "    # Fetch the international fixtures page\n",
    "    fixtures_page = session.get(fixtures_link)\n",
    "    fixtures_soup = BeautifulSoup(fixtures_page.content, 'html.parser')\n",
    "\n",
    "    # Extract fixtures details\n",
    "    fixtures = fixtures_soup.find_all('div', class_='js-list')\n",
    "    \n",
    "    # Initialize an empty list to store the fixtures details\n",
    "    fixtures_details = []\n",
    "\n",
    "    for fixture in fixtures:\n",
    "        try:\n",
    "            series = fixture.find('div', class_='fixture__tournament-label').text.strip()\n",
    "            place = fixture.find('div', class_='fixture__venue').text.strip()\n",
    "            date = fixture.find('div', class_='fixture__full-date').text.strip()\n",
    "            time = fixture.find('span', class_='fixture__time').text.strip()\n",
    "            \n",
    "            fixtures_details.append({\n",
    "                'Series': series,\n",
    "                'Place': place,\n",
    "                'Date': date,\n",
    "                'Time': time\n",
    "            })\n",
    "        except AttributeError:\n",
    "            # If any element is not found, skip the fixture\n",
    "            continue\n",
    "\n",
    "    # Print the extracted details\n",
    "    for detail in fixtures_details:\n",
    "        print(f\"Series: {detail['Series']}\")\n",
    "        print(f\"Place: {detail['Place']}\")\n",
    "        print(f\"Date: {detail['Date']}\")\n",
    "        print(f\"Time: {detail['Time']}\")\n",
    "        print('-' * 30)\n",
    "else:\n",
    "    print(\"International fixtures link not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc50273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"http://statisticstimes.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the economy page link\n",
    "economy_page_link = soup.find('a', string='Economy')\n",
    "if economy_page_link is not None:\n",
    "    economy_url = url + economy_page_link['href']\n",
    "    economy_response = requests.get(economy_url)\n",
    "\n",
    "    # Parse the HTML content of the economy page\n",
    "    economy_soup = BeautifulSoup(economy_response.content, 'html.parser')\n",
    "\n",
    "    # Find the table with state-wise GDP data\n",
    "    table = economy_soup.find('table', {'class': 'table table-striped table-bordered'})\n",
    "    if table is not None:\n",
    "        data = []\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            cols = row.find_all('td')\n",
    "            if cols is not None:\n",
    "                rank = cols[0].text.strip()\n",
    "                state = cols[1].text.strip()\n",
    "                gsdp_18_19 = cols[2].text.strip()\n",
    "                gsdp_19_20 = cols[3].text.strip()\n",
    "                share_18_19 = cols[4].text.strip()\n",
    "                gdp_billion = cols[5].text.strip()\n",
    "                data.append({\n",
    "                    'Rank': rank,\n",
    "                    'State': state,\n",
    "                    'GSDP(18-19)- at current prices': gsdp_18_19,\n",
    "                    'GSDP(19-20)- at current prices': gsdp_19_20,\n",
    "                    'Share(18-19)': share_18_19,\n",
    "                    'GDP($ billion)': gdp_billion\n",
    "                })\n",
    "\n",
    "        # Print the extracted data\n",
    "        for item in data:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d2d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Title: Unknown\n",
      "Repository Description: 18 Lessons, Get Started Building with Generative AI üîó https://microsoft.github.io/generative-ai-for-beginners/\n",
      "Contributors Count: Unknown\n",
      "Language: Jupyter Notebook\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: 30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw\n",
      "Contributors Count: Unknown\n",
      "Language: Python\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: Python toolkit for quantitative finance\n",
      "Contributors Count: Unknown\n",
      "Language: Jupyter Notebook\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: üîç A Hex Editor for Reverse Engineers, Programmers and people who value their retinas when working at 3 AM.\n",
      "Contributors Count: Unknown\n",
      "Language: C++\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: Elden Ring Save Editor. Compatible with PC and Playstation saves.\n",
      "Contributors Count: Unknown\n",
      "Language: Rust\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: A WhatsApp client library for NodeJS that connects through the WhatsApp Web browser app\n",
      "Contributors Count: Unknown\n",
      "Language: JavaScript\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: Learning English through the method of constructing sentences with conjunctions\n",
      "Contributors Count: Unknown\n",
      "Language: TypeScript\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: Community curated list of templates for the nuclei engine to find security vulnerabilities.\n",
      "Contributors Count: Unknown\n",
      "Language: JavaScript\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: Actual's server\n",
      "Contributors Count: Unknown\n",
      "Language: JavaScript\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: The Moby Project - a collaborative project for the container ecosystem to assemble container-based systems\n",
      "Contributors Count: Unknown\n",
      "Language: Go\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: ‚òÑüååÔ∏è The minimal, blazing-fast, and infinitely customizable prompt for any shell!\n",
      "Contributors Count: Unknown\n",
      "Language: Rust\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: This repository is for active development of the Azure SDK for .NET. For consumers of the SDK we recommend visiting our public developer docs at https://learn.microsoft.com/dotnet/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-net.\n",
      "Contributors Count: Unknown\n",
      "Language: C#\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: Self-hosted game stream host for Moonlight.\n",
      "Contributors Count: Unknown\n",
      "Language: C++\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: A framework for building native applications using React\n",
      "Contributors Count: Unknown\n",
      "Language: C++\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: AWS zero to hero repo for devops engineers to learn AWS in 30 Days. This repo includes projects, presentations, interview questions and real time examples.\n",
      "Contributors Count: Unknown\n",
      "Language: Python\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: Fast, secure, efficient backup program\n",
      "Contributors Count: Unknown\n",
      "Language: Go\n",
      "-------------------------------\n",
      "Repository Title: Unknown\n",
      "Repository Description: NVIDIA GPU Operator creates/configures/manages GPUs atop Kubernetes\n",
      "Contributors Count: Unknown\n",
      "Language: Go\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/trending\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "repositories = soup.find_all('article', class_='Box-row')\n",
    "\n",
    "for repository in repositories:\n",
    "    title_element = repository.find('h1', class_='lh-condensed')\n",
    "    if title_element:\n",
    "        title = title_element.text.strip()\n",
    "    else:\n",
    "        title = \"Unknown\"\n",
    "\n",
    "    description_element = repository.find('p', class_='col-9')\n",
    "    if description_element:\n",
    "        description = description_element.text.strip()\n",
    "    else:\n",
    "        description = \"Unknown\"\n",
    "\n",
    "    contributors_element = repository.find('span', class_='text-gray')\n",
    "    if contributors_element:\n",
    "        contributors_count = contributors_element.text.strip()\n",
    "    else:\n",
    "        contributors_count = \"Unknown\"\n",
    "\n",
    "    language_element = repository.find('span', itemprop='programmingLanguage')\n",
    "    if language_element:\n",
    "        language = language_element.text.strip()\n",
    "    else:\n",
    "        language = \"Unknown\"\n",
    "\n",
    "    print(f\"Repository Title: {title}\")\n",
    "    print(f\"Repository Description: {description}\")\n",
    "    print(f\"Contributors Count: {contributors_count}\")\n",
    "    print(f\"Language: {language}\")\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af9ab21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://www.billboard.com/charts/hot-100'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "song_list = soup.find_all('tr', {'class': 'chart-list-item'})\n",
    "\n",
    "for song in song_list[1:]:\n",
    "    song_name_element = song.find('td', {'class': 'chart-list-item__title'})\n",
    "    if song_name_element:\n",
    "        song_name = song_name_element.text.strip()\n",
    "    else:\n",
    "        song_name = 'Unknown'\n",
    "\n",
    "    artist_name_element = song.find('td', {'class': 'chart-list-item__artist'})\n",
    "    if artist_name_element:\n",
    "        artist_name = artist_name_element.text.strip()\n",
    "    else:\n",
    "        artist_name = 'Unknown'\n",
    "\n",
    "    last_week_rank_element = song.find('td', {'class': 'chart-list-item__last-week'})\n",
    "    if last_week_rank_element:\n",
    "        last_week_rank = last_week_rank_element.text.strip()\n",
    "    else:\n",
    "        last_week_rank = 'Unknown'\n",
    "\n",
    "    peak_rank_element = song.find('td', {'class': 'chart-list-item__peak'})\n",
    "    if peak_rank_element:\n",
    "        peak_rank = peak_rank_element.text.strip()\n",
    "    else:\n",
    "        peak_rank = 'Unknown'\n",
    "\n",
    "    weeks_on_board_element = song.find('td', {'class': 'chart-list-item__weeks'})\n",
    "    if weeks_on_board_element:\n",
    "        weeks_on_board = weeks_on_board_element.text.strip()\n",
    "    else:\n",
    "        weeks_on_board = 'Unknown'\n",
    "\n",
    "    song_details.append({\n",
    "        'Song Name': song_name,\n",
    "        'Artist Name': artist_name,\n",
    "        'Last Week Rank': last_week_rank,\n",
    "        'Peak Rank': peak_rank,\n",
    "        'Weeks on Board': weeks_on_board\n",
    "    })\n",
    "\n",
    "print(song_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a40e05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "book_list = soup.find_all('div', {'class': 'content__article-body from-content-api js-article__body'})\n",
    "\n",
    "book_details = []\n",
    "\n",
    "for book in book_list:\n",
    "    book_name = book.find('h2').text.strip()\n",
    "    author_name = book.find('p').text.strip()\n",
    "    volumes_sold = book.find('span', {'class': 'value'}).text.strip()\n",
    "    publisher = book.find('span', {'class': 'publisher'}).text.strip()\n",
    "    genre = book.find('span', {'class': 'genre'}).text.strip()\n",
    "\n",
    "    book_details.append({\n",
    "        'Book Name': book_name,\n",
    "        'Author Name': author_name,\n",
    "        'Volumes Sold': volumes_sold,\n",
    "        'Publisher': publisher,\n",
    "        'Genre': genre\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(book_details)\n",
    "df.to_csv('highest_selling_novels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63a1faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "lister_list = soup.find('div', class_='lister-list')\n",
    "if lister_list:\n",
    "    tv_series_list = lister_list.find_all('div', class_='lister-item')\n",
    "    for tv_series in tv_series_list:\n",
    "        name = tv_series.find('h3', class_='lister-item-header').a.text if tv_series.find('h3', class_='lister-item-header') and tv_series.find('h3', class_='lister-item-header').a else \"N/A\"\n",
    "        year_span = tv_series.find('h3', class_='lister-item-header').find('span', class_='lister-item-year').text.strip('()') if tv_series.find('h3', class_='lister-item-header') and tv_series.find('h3', class_='lister-item-header').find('span', class_='lister-item-year') else \"N/A\"\n",
    "        genre = tv_series.find('span', class_='genre').text.strip() if tv_series.find('span', class_='genre') else \"N/A\"\n",
    "        run_time = tv_series.find('span', class_='runtime').text.strip() if tv_series.find('span', class_='runtime') else \"N/A\"\n",
    "        ratings = tv_series.find('span', class_='ipl-rating-star__rating').text if tv_series.find('span', class_='ipl-rating-star__rating') else \"N/A\"\n",
    "        votes = tv_series.find('span', class_='ipl-rating-star__total-votes').text.strip('Votes: ') if tv_series.find('span', class_='ipl-rating-star__total-votes') else \"N/A\"\n",
    "\n",
    "        print(\"Name:\", name)\n",
    "        print(\"Year Span:\", year_span)\n",
    "        print(\"Genre:\", genre)\n",
    "        print(\"Run Time:\", run_time)\n",
    "        print(\"Ratings:\", ratings)\n",
    "        print(\"Votes:\", votes)\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d8c89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Go to the Show All Dataset page\n",
    "show_all_datasets_link = soup.find('a', text='Show All Datasets')\n",
    "if show_all_datasets_link:\n",
    "    show_all_datasets_link = show_all_datasets_link['href']\n",
    "    show_all_datasets_url = url + show_all_datasets_link\n",
    "    response = requests.get(show_all_datasets_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract dataset details\n",
    "    datasets = soup.find_all('tr', class_='dataset')\n",
    "    for dataset in datasets:\n",
    "        dataset_name_td = dataset.find('td', class_='dataset-name')\n",
    "        data_type_td = dataset.find('td', class_='dataset-type')\n",
    "        task_td = dataset.find('td', class_='dataset-task')\n",
    "        attribute_type_td = dataset.find('td', class_='dataset-attribute-type')\n",
    "        num_instances_td = dataset.find('td', class_='dataset-num-instances')\n",
    "        num_attributes_td = dataset.find('td', class_='dataset-num-attributes')\n",
    "        year_td = dataset.find('td', class_='dataset-year')\n",
    "\n",
    "        dataset_name = dataset_name_td.text.strip() if dataset_name_td else \"N/A\"\n",
    "        data_type = data_type_td.text.strip() if data_type_td else \"N/A\"\n",
    "        task = task_td.text.strip() if task_td else \"N/A\"\n",
    "        attribute_type = attribute_type_td.text.strip() if attribute_type_td else \"N/A\"\n",
    "        num_instances = num_instances_td.text.strip() if num_instances_td else \"N/A\"\n",
    "        num_attributes = num_attributes_td.text.strip() if num_attributes_td else \"N/A\"\n",
    "        year = year_td.text.strip() if year_td else \"N/A\"\n",
    "\n",
    "        print(\"Dataset Name:\", dataset_name)\n",
    "        print(\"Data Type:\", data_type)\n",
    "        print(\"Task:\", task)\n",
    "        print(\"Attribute Type:\", attribute_type)\n",
    "        print(\"No of Instances:\", num_instances)\n",
    "        print(\"No of Attributes:\", num_attributes)\n",
    "        print(\"Year:\", year)\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6023e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
