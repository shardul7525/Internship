{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    url = f\"https://www.amazon.in/s?k={product}\"\n",
    "    headers = ({'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "    webpage = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "    products = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "    for product in products:\n",
    "        title = product.find(\"span\", {\"class\": \"a-size-medium a-color-base a-text-normal\"}).string\n",
    "        price = product.find(\"span\", {\"class\": \"a-price-whole\"}).string\n",
    "        rating = product.find(\"span\", {\"class\": \"a-icon-alt\"}).string\n",
    "        review_count = product.find(\"span\", {\"class\": \"a-size-base\"}).string\n",
    "        available = product.find(\"span\", {\"class\": \"a-color-success\"}).string\n",
    "\n",
    "        print(\"Product Title = \", title)\n",
    "        print(\"Product Price = \", price)\n",
    "        print(\"Product Rating = \", rating)\n",
    "        print(\"Total Reviews = \", review_count)\n",
    "        print(\"Availability = \", available)\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "product = input(\"Enter the product to search: \")\n",
    "search_amazon(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def search_amazon(product):\n",
    "    url = f\"https://www.amazon.in/s?k={product}\"\n",
    "    headers = ({'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d0e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set up Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Set up Chrome driver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Keywords to search\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Create a directory to save the images\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "for keyword in keywords:\n",
    "    # Navigate to Google Images\n",
    "    driver.get(\"https://images.google.com/\")\n",
    "\n",
    "    # Find the search bar\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "\n",
    "    # Enter the keyword\n",
    "    search_bar.send_keys(keyword)\n",
    "\n",
    "    # Find the search button\n",
    "    search_button = driver.find_element_by_name(\"btnG\")\n",
    "\n",
    "    # Click the search button\n",
    "    search_button.click()\n",
    "\n",
    "    # Wait for the images to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Find the images\n",
    "    images = driver.find_elements_by_css_selector(\"img.rg_i\")\n",
    "\n",
    "    # Create a directory for the keyword\n",
    "    if not os.path.exists(f'images/{keyword}'):\n",
    "        os.makedirs(f'images/{keyword}')\n",
    "\n",
    "    # Save the images\n",
    "    for i, image in enumerate(images[:10]):\n",
    "        src = image.get_attribute(\"src\")\n",
    "        if src:\n",
    "            img_data = requests.get(src).content\n",
    "            with open(f'images/{keyword}/{i+1}.jpg', 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "\n",
    "    # Go back to the search page\n",
    "    driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "# Close the Chrome driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_smartphone_details(smartphone_name):\n",
    "    url = f\"https://www.flipkart.com/search?q={smartphone_name}\"\n",
    "    headers = ({'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.3'})\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    products = soup.find_all('div', {'class': '_1AtVbE col-12-12'})\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for product in products:\n",
    "        brand_name = product.find('div', {'class': '_2WkVRV'}).text.strip() if product.find('div', {'class': '_2WkVRV'}) else '-'\n",
    "        smartphone_name = product.find('div', {'class': '_4rR01T'}).text.strip() if product.find('div', {'class': '_4rR01T'}) else '-'\n",
    "        colour = product.find('div', {'class': 'col col-7-12'}).find_all('li')[0].text.strip() if product.find('div', {'class': 'col col-7-12'}) else '-'\n",
    "        ram = product.find('div', {'class': 'col col-7-12'}).find_all('li')[1].text.strip() if product.find('div', {'class': 'col col-7-12'}) else '-'\n",
    "        storage = product.find('div', {'class': 'col col-7-12'}).find_all('li')[2].text.strip() if product.find('div', {'class': 'col col-7-12'}) else '-'\n",
    "        primary_camera = product.find('div', {'class': 'col col-7-12'}).find_all('li')[3].text.strip() if product.find('div', {'class': 'col col-7-12'}) else '-'\n",
    "        secondary_camera = product.find('div', {'class': 'col col-7-12'}).find_all('li')[4].text.strip() if product.find('div', {'class': 'col col-7-12'}) else '-'\n",
    "        display_size = product.find('div', {'class': 'col col-7-12'}).find_all('li')[5].text.strip() if product.find('div', {'class': 'col col-7-12'}) else '-'\n",
    "        battery_capacity = product.find('div', {'class': 'col col-7-12'}).find_all('li')[6].text.strip() if product.find('div', {'class': 'col col-7-12'}) else '-'\n",
    "        price = product.find('div', {'class': '_30jeq3 _1_WHN1'}).text.strip() if product.find('div', {'class': '_30jeq3 _1_WHN1'}) else '-'\n",
    "        product_url = 'https://www.flipkart.com' + product.find('a')['href'] if product.find('a') else '-'\n",
    "\n",
    "        data.append({\n",
    "            'Brand Name': brand_name,\n",
    "            'Smartphone Name': smartphone_name,\n",
    "            'Colour': colour,\n",
    "            'RAM': ram,\n",
    "            'Storage(ROM)': storage,\n",
    "            'Primary Camera': primary_camera,\n",
    "            'Secondary Camera': secondary_camera,\n",
    "            'Display Size': display_size,\n",
    "            'Battery Capacity': battery_capacity,\n",
    "            'Price': price,\n",
    "            'Product URL': product_url\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f'{smartphone_name}.csv', index=False)\n",
    "\n",
    "# Test the function\n",
    "scrape_smartphone_details('Oneplus Nord')\n",
    "scrape_smartphone_details('Pixel 4A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899552e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_geospatial_coordinates(city):\n",
    "    url = f\"https://www.google.com/maps/search/{city}\"\n",
    "    headers = ({'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.3'})\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    script_tag = soup.find('script', {'type': 'application/json'})\n",
    "    data = script_tag.text\n",
    "    data = data.replace('window.APP_INITIALIZATION_STATE=', '')\n",
    "    data = json.loads(data)\n",
    "\n",
    "    latitude = data['bootstrapData']['locations'][0]['latitude']\n",
    "    longitude = data['bootstrapData']['locations'][0]['longitude']\n",
    "\n",
    "    return latitude, longitude\n",
    "\n",
    "city = input(\"Enter the city: \")\n",
    "latitude, longitude = scrape_geospatial_coordinates(city)\n",
    "print(f\"Latitude: {latitude}, Longitude: {longitude}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f010c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/laptops/best-gaming-laptops.html\"\n",
    "    headers = ({'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.3'})\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    laptops = soup.find_all('div', {'class': 'list-item'})\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for laptop in laptops:\n",
    "        name = laptop.find('h2', {'class': 'list-item-title'}).text.strip()\n",
    "        price = laptop.find('span', {'class': 'price'}).text.strip()\n",
    "        rating = laptop.find('span', {'class': 'rating'}).text.strip()\n",
    "        specs = laptop.find('ul', {'class': 'list-item-specs'}).find_all('li')\n",
    "        specs_data = []\n",
    "        for spec in specs:\n",
    "            specs_data.append(spec.text.strip())\n",
    "        data.append({\n",
    "            'Name': name,\n",
    "            'Price': price,\n",
    "            'Rating': rating,\n",
    "            'Specifications': specs_data\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('gaming_laptops.csv', index=False)\n",
    "\n",
    "scrape_gaming_laptops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e005cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/?sh=27551f5d66f6\"\n",
    "    headers = ({'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.3'})\n",
    "\n",
    "    data = []\n",
    "\n",
    "    page = 1\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        billionaires = soup.find_all('div', {'class': 'person'})\n",
    "\n",
    "        for billionaire in billionaires:\n",
    "            rank = billionaire.find('div', {'class': 'rank'}).text.strip()\n",
    "            name = billionaire.find('div', {'class': 'personName'}).text.strip()\n",
    "            net_worth = billionaire.find('div', {'class': 'netWorth'}).text.strip()\n",
    "            age = billionaire.find('div', {'class': 'age'}).text.strip()\n",
    "            citizenship = billionaire.find('span', {'class': 'country'}).text.strip()\n",
    "            source = billionaire.find('div', {'class': 'ource'}).text.strip()\n",
    "            industry = billionaire.find('div', {'class': 'industry'}).text.strip()\n",
    "\n",
    "            data.append({\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Net Worth': net_worth,\n",
    "                'Age': age,\n",
    "                'Citizenship': citizenship,\n",
    "                'Source': source,\n",
    "                'Industry': industry\n",
    "            })\n",
    "\n",
    "        next_page = soup.find('a', {'class': 'next'})\n",
    "        if next_page:\n",
    "            url = 'https://www.forbes.com' + next_page['href']\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('billionaires.csv', index=False)\n",
    "\n",
    "scrape_billionaires()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c91d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set up YouTube API credentials\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "CHANNEL_ID = \"YOUR_CHANNEL_ID\"\n",
    "\n",
    "# Set up YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# Set up video ID\n",
    "VIDEO_ID = \"VIDEO_ID_HERE\"\n",
    "\n",
    "# Set up comment extraction parameters\n",
    "MAX_COMMENTS = 500\n",
    "COMMENT_SORT_ORDER = \"relevance\"\n",
    "\n",
    "# Extract comments\n",
    "comments = []\n",
    "next_page_token = \"\"\n",
    "\n",
    "while len(comments) < MAX_COMMENTS:\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=VIDEO_ID,\n",
    "        maxResults=100,\n",
    "        pageToken=next_page_token,\n",
    "        order=COMMENT_SORT_ORDER\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    for item in response[\"items\"]:\n",
    "        comment = item[\"snippet\"][\"topLevelComment\"]\n",
    "        comments.append({\n",
    "            \"comment\": comment[\"snippet\"][\"textDisplay\"],\n",
    "            \"upvotes\": comment[\"snippet\"][\"likeCount\"],\n",
    "            \"posted_at\": comment[\"snippet\"][\"publishedAt\"]\n",
    "        })\n",
    "\n",
    "    next_page_token = response.get(\"nextPageToken\")\n",
    "\n",
    "    if not next_page_token:\n",
    "        break\n",
    "\n",
    "# Save comments to file\n",
    "with open(\"comments.json\", \"w\") as f:\n",
    "    json.dump(comments, f, indent=4)\n",
    "\n",
    "print(f\"Extracted {len(comments)} comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels():\n",
    "    url = \"https://www.hostelworld.com/hostels/London?dateFrom=2023-03-01&dateTo=2023-03-02&numberOfGuests=1&page=\"\n",
    "    headers = ({'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.3'})\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for page in range(1, 11):\n",
    "        response = requests.get(url + str(page), headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        hostels = soup.find_all('div', {'class': 'hostel-card-container'})\n",
    "\n",
    "        for hostel in hostels:\n",
    "            name = hostel.find('h3', {'class': 'hostel-name'}).text.strip()\n",
    "            distance = hostel.find('span', {'class': 'distance'}).text.strip()\n",
    "            rating = hostel.find('span', {'class': 'rating'}).text.strip()\n",
    "            total_reviews = hostel.find('span', {'class': 'reviews'}).text.strip()\n",
    "            overall_reviews = hostel.find('span', {'class': 'overall-reviews'}).text.strip()\n",
    "            privates_from_price = hostel.find('span', {'class': 'private-from'}).text.strip()\n",
    "            dorms_from_price = hostel.find('span', {'class': 'dorm-from'}).text.strip()\n",
    "\n",
    "            facilities = []\n",
    "            facility_tags = hostel.find_all('span', {'class': 'facility-tag'})\n",
    "            for facility in facility_tags:\n",
    "                facilities.append(facility.text.strip())\n",
    "\n",
    "            property_description = hostel.find('div', {'class': 'property-description'}).text.strip()\n",
    "\n",
    "            data.append({\n",
    "                'Name': name,\n",
    "                'Distance': distance,\n",
    "                'Rating': rating,\n",
    "                'Total Reviews': total_reviews,\n",
    "                'Overall Reviews': overall_reviews,\n",
    "                'Privates From Price': privates_from_price,\n",
    "                'Dorms From Price': dorms_from_price,\n",
    "                'Facilities': facilities,\n",
    "                'Property Description': property_description\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('hostels.csv', index=False)\n",
    "\n",
    "scrape_hostels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fc4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
